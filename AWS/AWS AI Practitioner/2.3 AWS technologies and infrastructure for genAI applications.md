## AWS Infrastructure and Technologies for Building Generative AI Applications

### **Accelerating AI Training with Transfer Learning**
- Training large language models (LLMs) involves processing vast amounts of data and millions of calculations, making the process resource-intensive and time-consuming.
- **Transfer Learning** enables faster and more accurate results by fine-tuning a pre-trained model on a smaller, task-specific dataset. This reduces the need for extensive data and training time.
- Pre-trained models can either be created in-house or sourced externally. They serve as a foundation, allowing the training process to focus on mapping new dataset elements to the pre-existing generalized knowledge.
### Sagemaker JumpStart
- SageMaker JumpStart helps to find projects that are already built and are quick builds with datasets, models, algorithm types, and solutions based on industry best practices. This task statement also includes understanding the benefits of AWS infrastructure for generative AI applications, especially security, compliance, responsibility, and safety. In task statement 2.2, we talked about customer experiences using your generative AI applications. But customers are also building generative AI applications by using large language models, LLMs, and other foundation models, FMs, to enhance experiences. These models can also transform operations, improve employee productivity, and create new revenue.

## Security and Compliance in GenAI

### Protecting Sensitive Data
- GenAI applications often handle sensitive business data, including personal, compliance, operational, and financial information.
- AWS emphasizes security across the three layers of the GenAI stack:
    - **Infrastructure Layer:** Ensures secure building and training of LLMs and other foundation models. This includes handling the compute-intensive demands of AI training and inference.
    - **Platform Layer:** Provides tools to develop, deploy, and scale AI/ML services securely, including the use of foundation models.
    - **Application Layer:** Focuses on secure consumption of models, including applications for tasks like content generation, insight derivation, and prompt engineering.
### AWS Nitro 
- The **AWS Nitro System** enhances security by enforcing restrictions to prevent unauthorized access to workloads and data on Amazon EC2 instances. Securing AI infrastructure means zero access to sensitive AI data, such as AI model weights and data processed with those models by any unauthorized person. 
- Nitro-based instances include specialized hardware and ML accelerators, such as **AWS Inferentia**, **AWS Trainium**, and GPU-based instances (P4, P5, G5, G6), offering better price-performance for AI workloads. 

- The middle layer provides access to all the models, along with tools that you need to build and scale generative AI applications. You can design how your platform supports the development, deployment, and iteration of AWS ML and AI services.  For example, ML services train or tune foundation models. And in this layer, all services consume models and capabilities, such as the large and costly to train foundation models in the generative AI domain.

- The top layer includes applications that use LLMs and other FMs to write and debug code, generate content, derive insights, and take actions. It might be a dashboard or a foundation model that you can use for prompt engineering, or it might be a specific generative AI architecture, such as Retrieval-Augmented Generation, RAG, applications. What are the three critical components of any AI system? It's input, model, and output. These components can be protected by establishing security policies, standards, and guidelines, along with the roles and responsibilities related to AI workloads. AI systems can have specific vulnerabilities, such as prompt injection, data poisoning, and model inversion vulnerabilities.

Remember to validate policies, because risks associated with an AI can have consequences, including privacy breaches, data manipulation, abuse, and compromised decision-making. Ensure that you understand why it's important to implement encryption, multi-factor authentication, continuous monitoring, and alignment to your tolerance and frameworks.

## Cost tradeoffs for AWS genAI services
### Pricing Models for Large Language Models (LLMs)
There are several pricing models to consider when working with Large Language Models (LLMs). One option is to host the LLMs on your own infrastructure. In this model, you are responsible for paying for the computing resources required to run the models, in addition to any potential license fees for the LLM itself.

Another option is to use a **token-based pricing model**. In this model, pricing is based on the number of tokens processed. Tokens are discrete units of information, which can represent a variety of things depending on the context. For example, in text-based models, tokens may represent characters or words, while in image-based models, tokens could represent pixels. Tokens are used by vendors to calculate the costs for API calls.

Using the token-based pricing model, such as with AWS, provides added scalability, as you only pay for the resources you use. In contrast, hosting your own model requires an upfront investment in infrastructure and ongoing maintenance costs.

### AWS Machine Learning Services
At the core of AWS's machine learning offerings is **Amazon SageMaker**, which provides tools for building, training, and deploying machine learning models at scale.

The AWS **machine learning layer** offers various managed services to help you build and deploy models. Above that, the **AI services** layer includes prebuilt algorithms and models, including generative AI services, which can be easily integrated into your applications. If you are familiar with APIs, coding, and the AWS SDK, you can use these services without needing expertise in machine learning or AI.

#### AWS Services for LLM-Based Applications
AWS provides several services to help you build applications using LLMs:
1. **SageMaker JumpStart**:
    - JumpStart is a model hub that allows you to quickly deploy foundation models and integrate them into your applications.
    - It offers fine-tuning and model deployment features, enabling you to quickly scale and move into production.
    - Resources like blogs, videos, and example notebooks are available to help you get started.
    - Note that JumpStart models require GPUs for fine-tuning and deployment. Always refer to the SageMaker pricing page before selecting compute resources and follow best practices to monitor costs. Additionally, ensure you delete unused model endpoints to avoid unnecessary charges.
2. **Amazon Bedrock**:
    - Amazon Bedrock is a fully managed AWS service that provides access to a variety of foundation models (FMs) via APIs.
    - The service includes models curated by AWS, as well as third-party models from providers like **Cohere** and **Stability AI**. This allows you to interact with different models for various use cases, significantly reducing the need to build your own models from scratch.
    - Bedrock supports the import of custom weights for different model architectures and allows you to serve custom models on-demand. This pay-per-use model eliminates the need for long-term commitments.
    - Amazon’s own **Titan Foundation Model** is available on Bedrock and serves as a general-purpose model with excellent capabilities in text generation.
    - Features like **playgrounds** and **model evaluations** help you experiment and choose the most suitable model for your use case. Playgrounds let you run model inference on different base foundation models and adjust inference parameters to optimize your results.

#### Cost Considerations and Trade-offs
When it comes to generative AI, most companies will not train their own LLMs because of the high cost in terms of resources, time, and research. Similarly, hosting your own LLM is generally not a viable option due to the significant hardware and storage expenses involved.

AWS provides a solution through **vector databases**, where data is stored as **embeddings** (vectors). These embeddings are compressed, indexed, and stored efficiently for advanced search capabilities. By leveraging AWS’s managed services, you can avoid the hefty investment in infrastructure and focus on building scalable generative AI applications.
#### Advantages of Using AWS for Generative AI
By utilizing AWS services, you can:
- **Build and scale generative AI applications** to deliver new customer and employee experiences.
- **Customize data** for your specific use cases, leveraging industry-leading foundation models (FMs) and LLMs.
- Utilize **generative AI-powered solutions** with enterprise-grade security and privacy protections.