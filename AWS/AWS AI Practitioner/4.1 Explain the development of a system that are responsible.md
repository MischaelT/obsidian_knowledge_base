## **Overview of Responsible AI and Core Dimensions**
Responsible AI refers to a set of guidelines and principles designed to ensure that AI systems operate in a safe, trustworthy, and ethical manner. By incorporating various principles, responsible AI aims to promote fairness, transparency, privacy, and more. Here are the core dimensions of a responsible AI model:

1. **Fairness**  
    Fairness ensures that AI models treat all individuals equitably, regardless of factors such as age, gender, ethnicity, or location. The goal is to avoid perpetuating or amplifying societal biases and discrimination in AI systems. Fairness is assessed by measuring bias and variance of outcomes across different demographic groups. Disparities can result in unequal treatment or outcomes for groups based on their characteristics, such as race or sex. Models must be developed to avoid such biases, which can also affect their accuracy.
2. **Explainability**  
    Explainability focuses on making AI decisions understandable to humans. This means providing clear reasoning for specific outcomes. For example, explaining why a loan application was rejected in understandable terms. Users need to trust AI decisions, which is why it’s essential to be able to explain them clearly and transparently.
3. **Robustness**  
    Robustness ensures AI systems can tolerate errors, failures, or unexpected inputs. It helps maintain trust in AI by minimizing mistakes, making sure systems perform reliably in varied conditions.
4. **Privacy and Security**  
    Privacy and security emphasize the protection of user data. This involves safeguarding personally identifiable information (PII) and ensuring that AI systems comply with data protection regulations to maintain user privacy.
5. **Governance**  
    Governance refers to the process of ensuring AI systems meet industry standards, are compliant with regulations, and effectively manage risk. Regular audits and proper estimation are important components of governance.
6. **Transparency**  
    Transparency involves making clear information available about the capabilities, limitations, and risks associated with an AI system. It also includes informing users when they are interacting with AI systems, enabling informed decision-making.
7. **Inclusivity and Diversity in Datasets**  
    Ethical AI systems require ethically sourced datasets that are inclusive, diverse, and free from bias. This means representing a broad range of perspectives, experiences, and demographic groups in training data. Models should be built on balanced datasets, ensuring equal representation and avoiding skewed distributions that could harm certain groups. A failure to include certain groups in training data can lead to negative outcomes or misrepresentation of their needs.

## Bias and Variance in AI Models
1. **Bias**  
    Bias in AI models can emerge due to class imbalances in training data. For example, if a dataset underrepresents certain groups, the model may perform better for the overrepresented group. This can result in biased predictions, such as inaccurately diagnosing diseases for women due to an imbalanced dataset.
2. **Overfitting and Underfitting**  
    Overfitting happens when a model becomes too specialized to its training data and fails to generalize well to real-world data, particularly for underrepresented groups. Underfitting occurs when the model doesn't learn enough from the data, leading to poor performance for certain groups. Both overfitting and underfitting can erode user trust and raise ethical concerns.

## **Ethical Considerations in Dataset Selection**
Ethical datasets are the foundation of responsible AI. They must demonstrate the following characteristics:
1. **Inclusivity**  
    Datasets should reflect a wide range of populations, perspectives, and experiences to avoid bias.
2. **Diversity**  
    It is essential to include a variety of attributes, features, and variables to avoid skewed outcomes.
3. **Curated Data Sources**  
    Carefully selected and varied data sources help ensure the quality and integrity of the dataset.
4. **Balanced Representation**  
    Datasets must ensure equal representation of different groups to avoid biased outcomes.
5. **Privacy Protection**  
    Datasets should safeguard sensitive information and adhere to privacy regulations, ensuring individuals' personal data is protected.
6. **Consent and Transparency**  
    Informed consent should be obtained from data subjects, with clear communication about how their data will be used.
7. **Regular Audits**  
    Periodic reviews of datasets are essential to identify and address potential biases or issues, ensuring continuous improvement.

---

## Ethical Practices in AI Model Selection

When selecting AI models, it is important to consider both technical and ethical aspects:
1. **Environmental Impact**  
    Training large models requires substantial compute resources, contributing to a significant carbon footprint. Consideration should be given to the environmental impact of AI systems, including carbon emissions and energy consumption.
2. **Sustainability**  
    Sustainability involves selecting AI models with minimal environmental impact and ensuring their long-term viability. Reusing existing work or pre-trained models can reduce the environmental cost of developing new models.
3. **Accountability**  
    Clear accountability must be established for AI model outcomes and decisions. Stakeholders should be involved in decisions to ensure the model’s ethical deployment and use.
4. **Stakeholder Engagement**  
    Engaging diverse stakeholders ensures that AI models reflect a broad range of perspectives and social responsibilities. Stakeholder involvement helps guide ethical decision-making and the selection of appropriate AI models.


## AWS services for development ethical and fair AI systems

### SageMaker Clarify
And let's talk about services and features from AWS that you can use to measure and monitor a model's bias, trustworthiness, and truthfulness. Biases are imbalances in data, or disparities in the performance of a model across different groups. 

SageMaker Clarify helps you mitigate bias by detecting potential bias during the data preparation, after model training, and in your deployed model, by examining specific attributes. A model's explainability is important to be certain that the decisions it makes are unbiased. SageMaker Clarify can improve explainability by looking at the inputs and outputs for your model, treating the model itself as a black box. By making these observations, it determines the relative importance of each feature. For example, it can say a loan application was rejected because two of the most important features, income and outstanding debt, did not meet the thresholds. Because SageMaker Clarify treats the model as a black box, it can understand the basis for how deep learning models are making predictions without understanding the inner workings. It can even explain computer vision and natural language processing models which are using unstructured data. SageMaker Clarify examines your dataset and model by using processing jobs. A SageMaker Clarify processing job uses the SageMaker Clarify processing container to interact with an Amazon S3 bucket. The S3 bucket would contain your input datasets, and a model that is deployed to a SageMaker inference endpoint. The SageMaker Clarify processing container obtains the input data set and configuration for analysis from an S3 bucket. For feature analysis, the SageMaker Clarify processing container sends requests to the model container, and retrieves model predictions from the response from the model container. After that step, the processing container computes and saves analysis results to the S3 bucket. These results include a JSON file with bias metrics, and global feature attributions, a visual report, and additional files for local feature attributions. You can download the results from the output location and view them. 

Here are a few examples of metrics that are measured by SageMaker Clarify when analyzing the training dataset. As we've seen, a balanced dataset is important to avoid errors that occur for a particular class. For example, an ML model trained primarily on data from middle-aged individuals might be less accurate when making predictions that involve younger and older people. Another type of imbalance occurs when the labels favor positive outcomes for one class over another. For our example, training data might exhibit an unwanted pattern in showing that loans are approved at a higher rate for middle-aged individuals. The demographic disparity metric indicates whether a particular class has a larger proportion of the rejected outcomes in the dataset than the accepted outcomes. For example, consider the case of college admissions. Women applicants comprised 46% of the rejected applicants, but made up only 32% of the accepted applicants. We say that this result shows demographic disparity, because the rate at which women were rejected exceeds the rate at which women were accepted. Here are some of the metrics that SageMaker Clarify looks at with a trained model. The difference in positive proportions in predictions metric indicates whether the model predicts positive outcomes differently for each class. This metric can be compared with the label imbalance in the training data. The goal is to see whether the bias in positive proportions changes after the training, or whether the bias is also present in the data. Remember that specificity measures how often the model correctly predicts a negative outcome. If the specificity is lower for middle-aged men than other age groups, the model is showing bias against the other age groups. The recall difference metric is the difference in recall of the model between two classes. Any difference in these recalls is a potential form of bias. Recall is the true positive rate, TPR, which measures how often the model correctly predicts the cases that should receive a positive outcome. If the recall rate is high for one class, but low for another, the difference provides a measure for this bias. Model accuracy can also be different between classes, and this difference is also a form of bias. The accuracy difference metric is the difference between the prediction accuracies for different classes. This result can occur when the data contains class imbalance. The treatment equality is the difference in the ratio of false negatives to false positives. Even if the accuracy of the model is the same for two classes, this ratio could have differences. A difference in the type of errors that occur for different classes can constitute bias. Consider the loan approval example, more incorrect loan denials for one class, and more incorrect loan approvals for another, are two very different outcomes that show bias between the classes. 

## Challenges and risks of working with generative AI models
Now, let's talk about the challenges and risks of working with generative AI models. Though all AI models are only as reliable as the data they're trained on, generative AI involves a risk of hallucination. This outcome occurs when the model makes up something that might sound factual, but is actually fiction. 

Hallucination is the result of the AI model attempting to fill in the gaps when something is missing in its training data. Depending on how the generated content is used, hallucinations can be disastrous. In 2023, lawyers submitted a brief that contained extracts and case citations which they got from generative AI to a New York court. However, the case and citations were fake. The court dismissed the client's case, sanctioned the lawyers, and fined them and their firm. Interestingly, AI generated works cannot be copyrighted because they are not the work of a human. However, a generative AI model might have been trained with data that's protected by copyrights, patents, or trademarks, which can then be included in outputs. Also, a user can submit a copyrighted work as an input, and the generative AI can create an unlicensed derivative. In 2023, Getty Images filed a lawsuit against the creators of Stable Diffusion, a text-to-image generative AI model. The lawsuit was for allegedly infringing on the use of more than 12 million photographs, their associated captions, and metadata in building the model. Biased model outputs can lead to discriminatory or unfair treatment of individuals or groups, which can become a major legal risk. The Equal Employment Opportunity Commission sued three companies for using an AI hiring program that discriminated against older applicants. The program automatically rejected applicants if the person was a female over age 55 or a male over age 60. Generative AI models are capable of generating content that is offensive, disturbing, or even obscene if that type of content was in their training data. Toxic content can cause real world harm to users such as mental and emotional health problems. It can even cause increased propensity toward violence against specific individuals or marginalized groups. Data privacy is a risk, because sensitive data that makes its way into a large language model can leak and be incorporated into its output. This output could include PII, intellectual property, trade secrets, and healthcare records to name a few. This information could end up in the model by being present in training data or input as a prompt by a user. One problem is that as soon as an FM is trained or sees some data, you can't make it forget by deleting the data. Any of these risks can result in the loss of customer trust and with reputational damage because of irresponsible AI practices or unintended consequences. Fortunately, for foundation models in Amazon Bedrock, you can configure guardrails to filter and block inappropriate content. You can use guardrails to define threshold for content filters for hate, insults, sexual content, or violence. You can also block topics altogether. For these topics, you can use plain text to describe the topics that should be denied. When a user submits a prompt, it must pass through guardrails first. If the prompt is not allowed by guardrails, the user receives a violation response that you configure. The prompt never makes it to the model. Guardrails can be set on both the prompt and the model response, so if a prompt passes the guardrail, the response can still be blocked. Otherwise it passes through as is.

Another feature in SageMaker Clarify is the ability to run evaluation jobs of large language models so that you can compare models. It can run four different types of tasks, including text generation, text classification, question and answering, and text summarization. It can then evaluate the performance for five different dimensions. Prompt stereotyping measures the probability of your model including biases in its response. It includes biases for race, gender, sexual orientation, religion, age, nationality, disability, physical appearance, and socioeconomic status. Toxicity checks your model for sexual references, rude, unreasonable, hateful, or aggressive comments, profanity, insults, flirtations, attacks on identities and threats. Factual knowledge checks the veracity of the model responses. Semantic robustness checks whether your model output changes because of keyword typos, random changes to uppercase, and random additions or deletions of white spaces. Accuracy compares the model output to the expected responses, such as classifying and summarizing the data correctly. You can use a built-in prompt dataset or supply your own. You can also get a human workforce such as employees or subject matter experts to provide their feedback. If you are using Amazon Bedrock, the same capabilities are available in the Bedrock console to help evaluate the pre-trained LLMs in Amazon Bedrock. 
