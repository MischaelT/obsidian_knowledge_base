## Interacting with AWS

1. **AWS Management Console:**
    - The web-based user interface that allows you to manage AWS services and resources easily.
    - Suitable for users who prefer a visual representation of their resources and are managing them on a smaller scale.
2. **AWS CLI (Command Line Interface):**
    - A powerful tool that allows you to interact with AWS services through command-line commands.
    - Useful for scripting and automating tasks. It’s also beneficial for managing resources efficiently and executing batch processes.
    - Make sure to note that the CLI can be installed on multiple platforms (Windows, macOS, Linux).
3. **SDKs (Boto3 for Python):**
    - #Boto3 is the AWS SDK for #Python, which allows you to write Python code to interact with AWS services.
    - It provides a Pythonic way to use AWS services, enabling developers to automate processes and build applications.
    - Other SDKs are available for different programming languages, such as Java (AWS SDK for #Java) and #JavaScript (AWS SDK for JavaScript).

## Tools for Automation

### The Declarative Approach
Using a declarative approach, you write code that declares the desired result of the task, rather than how to carry it out. For example, rather than using the AWS CLI to create a new virtual private cloud (VPC) and subnets, you could write declarative code that simply defines the configuration for the VPC and subnets. This approach naturally requires some intelligent software to figure out the operations required to achieve the desired result. CloudFormation, which is covered in the first part of this chapter, is the most well-known AWS service that takes a declarative approach to building and configuring your AWS infrastructure. In short, you write code containing the specifics of the AWS resources you want and present that code to CloudFormation, and it builds and configures those resources on your behalf in a matter of seconds.

### AWS CloudFormation
- A service that provides infrastructure as code (IaC) to define and provision AWS resources.
- You can create templates in YAML or JSON format to describe your cloud resources, allowing for version control and repeatable deployments.
- Great for setting up complex environments consistently and reliably. It enables the automation of resource provisioning and configuration.
- Templates The code that defines your resources is stored in text fi les called templates . Templates use the proprietary #CloudFormation language, which can be written in JavaScript Object Notation #JSON or #YAML format. Templates contain a description of the AWS resources you want CloudFormation to create, so they simultaneously function as infrastructure documentation. Because templates are fi les, you can store them in a version-controlled system such as an #S3 bucket or a Git repository, allowing you to track changes over time. You can use the same template to build AWS infrastructure repeatedly, such as for development or testing. You can also use a single template to build many similar environments. For example, you can use one template to create identical resources for both production and test environments. Both environments can have the same infrastructure— #VPC s, subnets, application load balancers, and so on—but they’ll be separate environments. CloudFormation uses random identifiers when naming resources to ensure the resources it creates don’t conflict with each other. You can write templates to optionally take parameter inputs, allowing you to customize your resources without modifying the original template. For instance, you can write a template that asks for a custom resource tag to apply to all the resources CloudFormation creates. You can make a parameter optional by specifying a default value, or you can require the user to specify a value in order to use the template.
#### Stacks
To provision resources from a template, you must specify a stack name that’s unique within your account. A stack is a container that organizes the resources described in the template. The purpose of a stack is to collectively manage related resources. If you delete a stack, CloudFormation automatically deletes all of the resources in it. This makes CloudFormation perfect for test and development environments that need to be provisioned as pristine and then thrown away when no longer needed. Deleting a stack helps ensure that all resources are deleted and that no forgotten resources are left lingering, running up charges.

#### Stack Updates
You can have CloudFormation change individual resources in a stack. Just modify the template to delete, modify, or add resources, and then instruct CloudFormation to perform a stack update using the template. CloudFormation will automatically update the resources accordingly. If any other resources are dependent upon a resource that you’ve updated, CloudFormation 193 CloudFormation will detect that and make sure those downstream resources are also reconfigured properly. Alternatively, you can create a change set. Make the desired changes to your template, and then have CloudFormation generate a change set based on the template. CloudFormation will let you review the specific changes it will make, and then you can decide whether to execute those changes or leave everything as is. CloudFormation makes it easy to update stacks, increasing the possibility that a less skilled user might inadvertently update a stack, making undesired changes to a critical resource. If you’re concerned about this possibility, you can create a stack policy to guard against accidental updates. A stack policy is a JSON document, separate from a template, that specifies what resources may be updated. You can use it prevent updates to any or all resources in a stack. If you absolutely need to update a stack, you can temporarily override the policy.

#### Tracking Stack
Changes Each stack contains a timestamped record of events that occur within the stack, including when resources were created, updated, or deleted. This makes it easy to see all changes made to your stack. It’s important to understand that resources in CloudFormation stacks can be changed manually, and CloudFormation doesn’t prevent this. For instance, you can manually delete a VPC that’s part of a CloudFormation stack. Drift detection is a feature that monitors your stacks for such changes and alerts you when they occur.

### AWS Developer Tools
The AWS Developer Tools are a collection of tools designed to help application developers develop, build, test, and deploy their applications onto EC2 and on-premises instances. These tools facilitate and automate the tasks that must take place to get a new application revision released into production. However, the AWS Developer Tools enable more than just application development. You can use them as part of any IaC approach to automate the deployment and configuration of your AWS infrastructure. 
#### CodeCommit
CodeCommit lets you create private Git repositories that easily integrate with other AWS services. Git (https://git-scm.com) is a version control system that you can use to store source code, CloudFormation templates, documents, or any arbitrary files—even binary files such as Amazon Machine Images (AMI) and Docker containers. These files are stored in a repository, colloquially known as a repo. Git uses a process called versioning, where all changes or commits to a repository are retained indefinitely, so you can always revert to an old version of a file if you need it. CodeCommit is useful for teams of people who need to collaborate on the same set of files, such as developers who collaborate on a shared source code base for an application. CodeCommit allows users to check out code by copying or cloning it locally to their machine. They make changes to it locally and then check it back in to the repository Git performs differencing to identify the differences between different versions of a fi le. If a developer makes a code change that breaks the application, differencing lets the developer see exactly what changed. They can then revert to the previous, working version of the fi le.
#### CodeBuild
CodeBuild is a fully managed build service. A build is a set of actions performed on source code to get it ready for deployment. A build could include testing the code for errors or transforming it into a machine-readable language, but the specifi c build actions depend on the application.
##### Build Actions
One of the primary purposes of the build process is to run tests against the new code to ensure it works properly. This could include tests to validate that the source code is formatted properly and doesn’t contain syntax errors. It may also involve testing the functionality of the code by simulating how a user might use it. Automated testing is a key part of a software development practice called continuous integration (CI). In CI, developers check their new or modifi ed code into a shared repository multiple times a day. A build provider, such as CodeBuild, runs tests against this code. This rapid testing and feedback provides early detection of bugs that the developer can fi x immediately. A build can also consist of compiling source code into a binary such as an executable or a package installer. You can even use CodeBuild to create Docker containers and Amazon Machine Images (AMIs). You defi ne your specifi c tasks in a build specifi cation fi le that must be included with the source code. CodeBuild can get source code from a CodeCommit, GitHub, or Bitbucket repository, or an S3 bucket. CodeBuild can perform multiple builds simultaneously. Any outputs or artifacts that CodeBuild creates are stored in an S3 bucket, making them accessible to the rest of your AWS environment.
##### CodeBuild
performs builds in an isolated build environment that runs on a compute instance. It creates the build environment before each build and discards it when the build is fi nished. This isolation ensures a consistent build process that isn’t affected by leftovers from previous builds. The build environment always consists of an operating system and a Docker image that can include a programming language runtime and tools. AWS offers preconfigured build environments for Java, Ruby, Python, Go, Node.js, Android, .NET Core, PHP, and Docker. You can also create your own custom build environment. You can choose from the following three different compute types for your build environment: 
✓■ build.general1.small—3 GB of memory and 2 vCPU 
✓■ build.general1.medium—7 GB of memory and 4 vCPU
✓■ build.general1.large—15 GB of memory and 8 vCPU
All of the compute types support Linux, while the medium and large types also support Windows.

#### CodeDeploy
CodeDeploy can automatically deploy applications to EC2 instances, the Elastic Container Service (ECS), Lambda, and even on-premises servers. CodeDeploy works by pulling source fi les from an S3 bucket, or a GitHub or Bitbucket repository. In addition to your application’s source fi les, you must provide an applicationspecifi cation fi le that contains information about how to deploy the application.
- The CodeDeploy service doesn’t offer the option to deploy from a CodeCommit repo. But don’t worry, as CodePipeline makes this possible. We’ll cover CodePipeline later in the chapter.
##### Deploying to EC2 or On-Premises Instances
CodeDeploy can deploy applications, scripts, confi guration fi les, and virtually any other fi le to an EC2 or on-premises instance. To deploy to an instance, you must install the CodeDeploy agent. The agent allows the CodeDeploy service to copy fi les to the instance and perform deployment tasks on it. The agent has been tested with current versions of the following: ✓■ Amazon Linux ✓■ Ubuntu Server ✓■ Microsoft Windows Server ✓■ Red Hat Enterprise Linux (RHEL) CodeDeploy gives you many options for how to carry out an application deployment. You can deploy to instances according to specifi c resource tags, based on Auto Scaling Group membership, or by just manually selecting them. You can also control the cadence of deployment by deploying to instances one at a time, all at once, half at a time, or anywhere in between.

When it comes to upgrade deployments, CodeDeploy supports two different deployment types: In-place deployment An in-place deployment deploys the application to existing instances. If your application is running, CodeDeploy can stop it, perform any needed cleanup, deploy the new version, and then restart the application. Blue/Green deployment With a blue/green deployment, CodeDeploy deploys your application to a new set of instances that you either create manually or have CodeDeploy create by replicating an existing Auto Scaling group. CodeDeploy then deploys the application to the new instances. It can also optionally decommission the old instances. If you’re using an elastic load balancer, CodeDeploy can also automatically redirect traffic to the new instances.

##### Deploying to ECS
The process for deploying to ECS is almost the same as for deploying to EC2 instances, except instead of deploying application files to an instance, you deploy Docker images that run your application in containers. If you’re doing a deployment upgrade of your application and using an application load balancer, CodeDeploy can shift the traffic from your old containers to your new ones. CodeDeploy works with both EC2 and Fargate ECS launch types.
##### Deploying to Lambda
Lambda is Amazon’s serverless computing platform that runs functions that can be written in a variety of languages. Because you don’t have to even think about servers when using Lambda, deploying a new Lambda application with CodeDeploy simply involves creating a new Lambda function. If you need to update an existing function, CodeDeploy just creates a new version of that function. You can then choose how CodeDeploy handles the switchover to the new version. You can have CodeDeploy shift traffic slowly from one version to the other, or you can do an immediate, full cutover to the new version.

#### CodePipeline
CodePipeline helps orchestrate and automate every task required to move software from development to production. It works by taking your source code and processing it through a series of stages, up to and including a deployment stage. In between the required source and deployment stages, you can add other stages also, such as a build stage that runs tests, or an approval stage that requires manual approval before deployment. CodePipeline enables automation of certain tasks that the respective services don’t offer on their own. For example, as noted earlier, CodeDeploy doesn’t allow deploying directly from a CodeCommit repository, but it does allow deploying from an S3 bucket. CodePipeline can automatically take an application in the CodeCommit repository and trigger CodeBuild to perform automated testing against it. Once the tests pass, CodePipeline packages up the application files and places them it into an S3 bucket. You
can then require manual approval before calling CodeDeploy to deploy the application to production. Deploying software this way is called continuous delivery. Each stage consists of one or more actions, and these actions can occur sequentially or in parallel. There are six types of actions that you can include in a pipeline: ■ Source ■ Build ■ Test ■ Approval ■ Deploy ■ Invoke With the exception of the Approval action, each action is performed by a provider that depending on the action can be an AWS or third-party service: Source providers Source providers include S3, CodeCommit, GitHub, and the Elastic Container registry (ECR) that stores Docker containers for Elastic Container Service. Build and test providers CodeBuild and third-party tools such as CloudBees, Jenkins, and TeamCity can provide building and testing services. Deploy providers CodePipeline supports a number of deploy providers. The most common ones you’re likely to see include the following: CloudFormation CodePipeline can automatically deploy your AWS infrastructure using CloudFormation. For example, developers can create their own template that builds a complete test environment. Whenever they update the template, CodePipeline can automatically deploy it to CloudFormation. This approach allows developers to create their own development infrastructure. CodeDeploy CodeDeploy can only source application files from GitHub or S3. But you can configure CodePipeline to pull files from CodeCommit, package them up, and put them in an S3 bucket for CodeDeploy to pick up and deploy. ECS CodePipeline can deploy Docker containers directly to ECS. By combining this with ECR for the source stage, you can use ECR as a source for images, rather than having to keep your images in an S3 bucket. S3 Suppose you have a website hosted in S3. You can keep the HTML and other files for your website in a CodeCommit repo for versioning. If you want to update your website, you make your changes in the repo. CodePipeline detects the change and copies the updates to your S3 bucket. Other supported deploy providers include Elastic Beanstalk, OpsWorks Stacks, the AWS Service Catalog, and the Alexa Skills Kit. Invoke The Invoke action invokes Lambda functions, and it works only with AWS Lambda. Approval You can insert manual approvals anywhere in the pipeline after the source stage.

EC2 Auto Scaling automatically launches preconfigured EC2 instances. The goal of Auto Scaling is to ensure you have just enough computing resources to meet user demand without over-provisioning. 

### EC2 Auto Scaling
EC2 Auto Scaling automatically launches preconfigured EC2 instances. The goal of Auto Scaling is to ensure you have just enough computing resources to meet user demand without over-provisioning.

"EC2 Auto Scaling fulfills one of the core principles of sound cloud architecture design: don’t guess your capacity needs. Auto Scaling can save money by reducing your capacity when you don’t need it and improve performance by increasing it when you do."
#### Launch Configurations and Launch Templates
Auto Scaling works by spawning instances from either a launch configuration or a launch template. For the purposes of Auto Scaling, both achieve the same basic purpose of defining the instance’s characteristics, such as AMI, disk configuration, and instance type. You can also install software and make custom configurations by placing commands into a Userdata script that automatically runs when Auto Scaling launches a new instance. But there are some differences between launch configurations and launch templates. Launch templates are newer and can be used to spawn EC2 instances manually, even without Auto Scaling. You can also modify them after you create them. Launch configurations, on the other hand, can be used only with Auto Scaling. And once you create a launch configuration, you can’t modify it.

#### Auto Scaling Groups
Instances created by Auto Scaling are organized into an Auto Scaling group. All instances in a group can be automatically registered with an application load balancer target group. The application load balancer distributes traffic to the instances, spreading the demand out evenly among them.

##### Desired Capacity
When you configure an Auto Scaling group, you define a desired capacity—the number of instances that you want Auto Scaling to create. Auto Scaling creates this many instances and strives to maintain the desired capacity. If you raise or lower the capacity, Auto Scaling launches or terminates instances to match.
##### Self-Healing
Failed instances are self-healing. If an instance fails or terminates, Auto Scaling re-creates a replacement. This way you always get the number of instances you expect. Auto Scaling can use EC2 or elastic load balancing (ELB) health checks to determine whether an instance is healthy. EC2 health checks consider the basic health of an instance, whether it’s running, and whether it has network connectivity. ELB health checks look at the health of the application running on an instance. An unhealthy instance is treated as a failed instance and is terminated, and Auto Scaling creates another in its place.

##### Scaling Actions
Scaling actions control when Auto Scaling launches or terminates instances. You control how many instances Auto Scaling launches or terminates by specifying a minimum and maximum group size. Auto Scaling will ensure the number of instances never goes outside of this range. Naturally, the desired capacity must rest within the minimum and maximum bounds. 
##### Dynamic Scaling
With dynamic scaling, Auto Scaling launches new instances in response to increased demand using a process called scaling out . It can also scale in, terminating instances when demand ceases. You can scale in or out according to a metric, such as average CPU utilization of your instances, or based on the number of concurrent application users. 
##### Scheduled Scaling
Instead of or in addition to dynamic scaling, Auto Scaling can scale in or out according to a schedule. This is particularly useful if your demand has predictable peaks and valleys. Predictive scaling is a feature that looks at historic usage patterns and predicts future peaks. It then automatically creates a scheduled scaling action to match. It needs at least one day’s worth of traffi c data to create a scaling schedule.

### Configuration Management

Configuration management is an approach to ensuring accurate and consistent confi guration of your systems. While automation is concerned with carrying out tasks, confi guration management is primarily concerned with enforcing and monitoring the internal confi guration state of your instances to ensure they’re what you expect. Such confi guration states primarily include but aren’t limited to operating system confi gurations and what software is installed. As with automation in general, confi guration management tools use either imperative or declarative approaches. AWS offers both approaches using two tools to help you achieve confi guration management of your EC2 and on-premises instances: Systems Manager and OpsWorks.

#### Systems Manager

Systems Manager uses the imperative approach to get your instances and AWS environment into the state that you want.

Command Documents Command documents are scripts that run once or periodically that get the system into the state you want. Using Command documents, you can install software on an instance, install the latest security patches, or take inventory of all software on an instance. You can use the same Bash or PowerShell commands you’d use with a Linux or Windows instance. Systems Manager can run these periodically, or on a trigger, such as a new instance launch. Systems Manager requires an agent to be installed on the instances that you want it to manage. Configuration Compliance Configuration Compliance is a feature of Systems Manager that can show you whether instances are in compliance with your desired configuration state—whether it’s having a certain version of an application installed or being up-to-date on the latest operating system security patches. Automation Documents In addition to providing configuration management for instances, Systems Manager lets you perform many administrative AWS operations you would otherwise perform using the AWS Management Console or the AWS CLI. These operations are defined using Automation documents. For example, you can use Systems Manager to automatically create a snapshot of an Elastic Block Store (EBS) volume, launch or terminate an instance, create a CloudFormation stack, or even create an AMI from an existing EBS volume. Distributor Using Systems Manager Distributor, you can deploy installable software packages to your instances. You create a .zip archive containing installable or executable software packages that your operating system recognizes, put the archive in an S3 bucket, and tell Distributor where to find it. Distributor takes care of deploying and installing the software. Distributor is especially useful for deploying a standard set of software packages that already come as installers or executables.

### OpsWorks
OpsWorks is a set of three different services that let you take a declarative approach to configuration management. As explained earlier in this chapter, using a declarative approach requires some intelligence to translate declarative code into imperative operations. OpsWorks uses two popular configuration management platforms that fulfill this requirement: Chef (https://www.chef.io) and Puppet Enterprise (https://puppet.com). Puppet and Chef are popular configuration management platforms that can configure operating systems, deploy applications, create databases, and perform just about any configuration task you can dream of, all using code. Both Puppet and Chef are widely used for on-premises deployments, but OpsWorks brings their power to AWS as the OpsWorks managed service.

Just as with automation in general, confi guration management is not an all-or-nothing decision. Thankfully, OpsWorks comes in three different fl avors to meet any confi guration management appetite. AWS OpsWorks for Puppet Enterprise and AWS OpsWorks for Chef Automate are robust and scalable options that let you run managed Puppet or Chef servers on AWS. This is good if you want to use confi guration management across all your instances. AWS OpsWorks Stacks provides a simple and fl exible approach to using confi guration management just for deploying applications. Instead of going all-in on confi guration management, you can just use it for deploying and confi guring applications. OpsWorks Stacks takes care of setting up the supporting infrastructure.

#### AWS OpsWorks for Puppet Enterprise and AWS OpsWorks for Chef Automate
The high-level architectures of AWS OpsWorks for Puppet Enterprise and Chef Automate are similar. They consist of at least one Puppet master server or Chef server to communicate with your managed nodes—EC2 or on-premises instances—using an installed agent. You defi ne the confi guration state of your instances—such as operating system confi gurations applications—using Puppet modules or Chef recipes. These contain declarative code, written in the platform’s domain-specifi c language, that specifi es the resources to provision. The code is stored in a central location, such as a Git repository like CodeCommit or an S3 bucket. OpsWorks manages the servers, but you’re responsible for understanding and operating the Puppet or Chef software, so you’ll need to know how they work and how to manage them. No worries, though, because both Chef and Puppet have large ecosystems brimming with off-the-shelf code that can be used for a variety of common scenarios. AWS OpsWorks Stacks If you like the IaC concept for your applications, but you aren’t familiar with managing Puppet or Chef servers, you can use AWS OpsWorks Stacks. OpsWorks Stacks lets you build your application infrastructure in stacks. A stack is a collection of all the resources your application needs: EC2 instances, databases, application load balancers, and so on. Each stack contains at least one layer, which is a container for some component of your application. To understand how you might use OpsWorks Stacks, consider a typical database-backed application that consists of three layers: ✓■ An application layer containing EC2 instances or containers ✓■ A database layer consisting of self-hosted or relational database service (RDS) database instances ✓■ An application load balancer that distributes traffic to the application layer There are two basic types of layers that OpsWorks uses: OpsWorks layers and service layers.

#### OpsWorks layers
An OpsWorks layer is a template for a set of instances. It specifies instance-level settings such as security groups and whether to use public IP addresses. It also includes an auto-healing option that automatically re-creates your instances if they fail. OpsWorks can also perform load-based or time-based auto scaling, adding more EC2 instances as needed to meet demand. An OpsWorks layer can provision Linux or Windows EC2 instances, or you can add existing Linux EC2 or on-premises instances to a stack. OpsWorks Stacks supports Amazon Linux, Ubuntu Server, CentOS, and Red Hat Enterprise Linux. To configure your instances and deploy applications, OpsWorks uses the same declarative Chef recipes as the Chef Automate platform, but it doesn’t provision a Chef server. Instead, OpsWorks Stacks performs configuration management tasks using the Chef Solo client that it installs on your instances automatically. Service layers A stack can also include service layers to extend the functionality of your stack to include other AWS services. Service layers include the following layers: Relational Database Service (RDS) Using an RDS service layer, you can integrate your application with an existing RDS instance. Elastic Load Balancing (ELB) If you have multiple instances in a stack, you can create an application load balancer to distribute traffic to them and provide high availability. Elastic Container Service (ECS) Cluster If you prefer to deploy your application to containers instead of EC2 instances, you can create an ECS cluster layer that connects your OpsWorks stack to an existing ECS cluster.