https://www.youtube.com/watch?v=jA9CpUSaSN4&ab_channel=%D0%9A%D0%BE%D0%BC%D0%BF%D1%8C%D1%8E%D1%82%D0%B5%D1%80%D0%BD%D1%8B%D0%B5%D0%BD%D0%B0%D1%83%D0%BA%D0%B8
 Машинное обучение. Метод опорных векторов. К.В. Воронцов, Школа анализа данных, Яндекс.

![[Pasted image 20230909121707.png]]

- Это расстояние от объекта x_i до разделяющей гиперплоскости. Если он лежит по правильную сторону от гиперплоскости, то знак плюс, если объект с ошибкой, то знак минус.

[[Empirical risk minimisation]]

![[Pasted image 20230909122044.png]]
Такми образом наша задача состоит в том, чтобы найти такую гиперплоскость чтобы как можно больше точек лежали в нужной полуплоскости.

![[Pasted image 20230909122519.png]]

ПРо факту задача классификации это задача поиска решения системы неравенств, но при этом если в изначальной выборке есть ошибки, то система неравентв будет несовместна. Таким образом мы приходим к иной задаче: задаче поиска максимальной подсистемы совместных неравенств. Это задача комбинаторного поиска, поэтому в общем случае она достаточно трудная. #svm  Это способ как решить такую задачу

В силу линейности модели эта система неравенств определена с точностью до нормировки

![[Pasted image 20230909124629.png]]

Для перехода к линейно неразделимой выборке мы вводим так называемые понятия slack variables
Чаще всего они используются для того, чтобы превратить неравенства в уравнения
Верхняя система к общем случае несовместна, тем не менее мы всегда можем подобрать кси и таким образом, чтобы сделать ее совместной

Функция которая стоит пв последней строчке кучосно-линейная, то есть для нее не получится использовать те методы, которае мы обыно используем для оптимизации гладкиз функций

НЕо в методе опорных векторов мы обычно решаем втогрую систему

#lagrangian #math 

![[Pasted image 20230909125701.png]]

![[Pasted image 20230909133639.png]]

![[Pasted image 20230909131903.png]]

- Из первого условия мы понимаем емук равен вектор решения
  Он выражается через дсойственные перем дябмда. По сути он является линейной комбинацией векторов обучающей выборкию Причем не от всех потому как лямбда может принимать и отрицательные значения, а соответственно от некоторых элементов обучайющей выборки вектор х вообще зависеть не будет
  Те объекты от которых решение жзависит называются опорными векторами 
![[Pasted image 20230909133912.png]]

Если мы подставим выражение для дубльв в лагранжиан, используя все те свойства, которые мы получили, то в результате у нас получится вот такое решение

![[Pasted image 20230909134746.png]]

Ограничения на лямбда определяют:
- первое это куб в пространстве размерности l
- второе это гиперплоскость
И пересечение этого куба гиперплоскостью это как раз то множество решений lambda, в котором мы ищем решение

Так как это выпуклая область (выпуклый куб, пересеченный гиперплоскостью) и мы ищем минимум квадратичного функционала (выпуклого), что гарантирует нам наличие единственного решения

Мы не приводим никаких численных методом для решения, потому как в общеи виде задача очень сложная и решать ее нужно с учетом вида функции.

последняя строчка это конечный линейный классфикатор. Некоторые лямбла равны нуль, те для которых это не так это опорные векторы. По сути мы сравниваем классифицируемые объекты с ними с помощью скалярного произведения. Мы используем скалярное произведение, как некую меру близости между объектами. В общем виде вместо скалярного произведения мы можем использовать любую другую меру близости

![[Pasted image 20230909141513.png]]
